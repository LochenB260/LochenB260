import requests
 
from bs4 import BeautifulSoup
 
import pandas as pd
 
 
# 收集单页数据 xpanx.com
 
def fetch_data(page_number):
    url = f"https://sh.lianjia.com/ershoufang/pg{page_number}/"
 
    response = requests.get(url)
 
    if response.status_code != 200:
        print("请求失败")
 
        return []
 
    soup = BeautifulSoup(response.text, 'html.parser')
 
    rows = []
 
    for house_info in soup.find_all("li", {"class": "clear LOGVIEWDATA LOGCLICKDATA"}):
        row = {}
 
        # 使用您提供的类名来获取数据 xpanx.com
 
        row['区域'] = house_info.find("div", {"class": "positionInfo"}).get_text() if house_info.find("div", {
            "class": "positionInfo"}) else None
 
        row['房型'] = house_info.find("div", {"class": "houseInfo"}).get_text() if house_info.find("div", {
            "class": "houseInfo"}) else None
 
        row['关注'] = house_info.find("div", {"class": "followInfo"}).get_text() if house_info.find("div", {
            "class": "followInfo"}) else None
 
        row['单价'] = house_info.find("div", {"class": "unitPrice"}).get_text() if house_info.find("div", {
            "class": "unitPrice"}) else None
 
        row['总价'] = house_info.find("div", {"class": "priceInfo"}).get_text() if house_info.find("div", {
            "class": "priceInfo"}) else None
 
        rows.append(row)
 
    return rows
 
 
# 主函数
 
def main():
    all_data = []
 
    for i in range(1, 11):  # 爬取前10页数据作为示例
 
        print(f"正在爬取第{i}页...")
 
        all_data += fetch_data(i)
 
    # 保存数据到Excel xpanx.com
 
    df = pd.DataFrame(all_data)
 
    df.to_excel('lianjia_data.xlsx', index=False)
 
    print("数据已保存到 'lianjia_data.xlsx'")
 
 
if __name__ == "__main__":
    main()
